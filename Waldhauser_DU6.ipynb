{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfaef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tavg  \tmin\tmax\n",
      "0  \t0     \t7.728\t1  \t50 \n",
      "1  \t392   \t9.879\t1  \t50 \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'fitness'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    149\u001b[39m     population = algorithms.eaSimple(population, \n\u001b[32m    150\u001b[39m                                      toolbox,\n\u001b[32m    151\u001b[39m                                      cxpb=CROSSOVER_RATE,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m                                      halloffame=hof,\n\u001b[32m    156\u001b[39m                                      verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# record max fitness for this generation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     max_fitness = \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpopulation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    159\u001b[39m     fitness_history.append(max_fitness)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# extract best individual\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    149\u001b[39m     population = algorithms.eaSimple(population, \n\u001b[32m    150\u001b[39m                                      toolbox,\n\u001b[32m    151\u001b[39m                                      cxpb=CROSSOVER_RATE,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m                                      halloffame=hof,\n\u001b[32m    156\u001b[39m                                      verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# record max fitness for this generation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     max_fitness = \u001b[38;5;28mmax\u001b[39m(\u001b[43mind\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitness\u001b[49m.values[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m population)\n\u001b[32m    159\u001b[39m     fitness_history.append(max_fitness)\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# extract best individual\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'fitness'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "\n",
    "# Environment configuration\n",
    "GRID_SIZE = 10              # 10x10 grid\n",
    "GOAL_POS = [9, 9]           # goal position\n",
    "OBSTACLES = [[3, 3], [3, 4], [4, 3], [4, 4], [7, 6], [6, 7]]  # optional obstacles\n",
    "MAX_STEPS = 50              # max steps per game round\n",
    "INPUT_SIZE = 6              # 6 sensors (4 directions to goal + 2 enhanced features)\n",
    "HIDDEN_SIZE = 10            # 10 neurons in hidden layer\n",
    "OUTPUT_SIZE = 4             # 4 outputs (up, down, left, right)\n",
    "GENOME_LENGTH = (INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE + \n",
    "                 HIDDEN_SIZE * OUTPUT_SIZE + OUTPUT_SIZE)  # 106 weights\n",
    "\n",
    "# Genetic algorithm parameters\n",
    "POPULATION_SIZE = 500       # number of individuals in population\n",
    "MUTATION_RATE = 0.2         # probability of mutation\n",
    "MUTATION_INDPB = 0.1        # probability of each gene being mutated\n",
    "CROSSOVER_RATE = 0.7        # probability of crossover\n",
    "TOURNAMENT_SIZE = 5         # size of tournament selection\n",
    "ELITISM_SIZE = 20           # number of best individuals to keep\n",
    "MAX_GENERATIONS = 200       # number of generations to run\n",
    "N_GAME_ROUNDS = 3           # number of game rounds to evaluate fitness\n",
    "\n",
    "# Clear existing DEAP classes to avoid conflicts\n",
    "if \"FitnessMax\" in creator.__dict__:\n",
    "    del creator.FitnessMax\n",
    "if \"Individual\" in creator.__dict__:\n",
    "    del creator.Individual\n",
    "\n",
    "# Create DEAP fitness and individual classes\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Enhanced sensory functions\n",
    "def sense_environment(pos, goal, obstacles=None):\n",
    "    \"\"\"\n",
    "    Return enhanced sensory inputs about the environment.\n",
    "    \n",
    "    Args:\n",
    "        pos: Current position [x, y]\n",
    "        goal: Goal position [x, y]\n",
    "        obstacles: List of obstacle positions (optional)\n",
    "        \n",
    "    Returns:\n",
    "        List of sensory inputs:\n",
    "        - Normalized distances to goal in 4 directions\n",
    "        - Distance to nearest obstacle (if any)\n",
    "        - Normalized Manhattan distance to goal\n",
    "    \"\"\"\n",
    "    x, y = pos\n",
    "    gx, gy = goal\n",
    "    \n",
    "    # 1-4. Distance to goal in each direction (normalized, closer = higher value)\n",
    "    dist_up = max(0, gy - y) / GRID_SIZE\n",
    "    dist_down = max(0, y - gy) / GRID_SIZE\n",
    "    dist_left = max(0, gx - x) / GRID_SIZE\n",
    "    dist_right = max(0, x - gx) / GRID_SIZE\n",
    "    \n",
    "    # 5. Normalized Manhattan distance to goal (closer = higher value)\n",
    "    manhattan_dist = (abs(x - gx) + abs(y - gy)) / (2 * GRID_SIZE)\n",
    "    manhattan_proximity = 1 - manhattan_dist  # invert: closer = higher value\n",
    "    \n",
    "    # 6. Obstacle detection (distance to nearest obstacle in agent's direction)\n",
    "    obstacle_sensor = 1.0  # default: no obstacle detected (normalized to [0,1])\n",
    "    \n",
    "    if obstacles:\n",
    "        # Find nearest obstacle (if any)\n",
    "        min_dist = float('inf')\n",
    "        for ox, oy in obstacles:\n",
    "            dist = ((x - ox) ** 2 + (y - oy) ** 2) ** 0.5  # Euclidean distance\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "        \n",
    "        # Normalize obstacle distance (closer = lower value)\n",
    "        if min_dist < GRID_SIZE:\n",
    "            obstacle_sensor = min(1.0, min_dist / GRID_SIZE)\n",
    "    \n",
    "    return [\n",
    "        1 - dist_up,        # closer = higher value \n",
    "        1 - dist_down,\n",
    "        1 - dist_left,\n",
    "        1 - dist_right,\n",
    "        manhattan_proximity,\n",
    "        obstacle_sensor\n",
    "    ]\n",
    "\n",
    "# Neural network function\n",
    "def nn_function(inp, wei):\n",
    "    \"\"\"\n",
    "    Implement neural network computation with ReLU activation in hidden layer.\n",
    "    \n",
    "    Args:\n",
    "        inp: List of sensory inputs\n",
    "        wei: Genome (weights) of neural network\n",
    "        \n",
    "    Returns:\n",
    "        List of output values (one per action)\n",
    "    \"\"\"\n",
    "    # Extract weights from genome\n",
    "    w_ih = np.array(wei[:INPUT_SIZE * HIDDEN_SIZE]).reshape(INPUT_SIZE, HIDDEN_SIZE)\n",
    "    b_h = np.array(wei[INPUT_SIZE * HIDDEN_SIZE:INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE])\n",
    "    w_ho = np.array(wei[INPUT_SIZE * HIDDEN_SIZE + HIDDEN_SIZE:-OUTPUT_SIZE]).reshape(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    b_o = np.array(wei[-OUTPUT_SIZE:])\n",
    "    \n",
    "    # Compute hidden layer (ReLU activation)\n",
    "    hidden = np.dot(inp, w_ih) + b_h\n",
    "    hidden = np.maximum(0, hidden)  # ReLU activation\n",
    "    \n",
    "    # Compute output layer (softmax for better probability distribution)\n",
    "    output = np.dot(hidden, w_ho) + b_o\n",
    "    \n",
    "    # Apply softmax to get probability distribution\n",
    "    exp_output = np.exp(output - np.max(output))  # Subtract max for numerical stability\n",
    "    output_probs = exp_output / exp_output.sum()\n",
    "    \n",
    "    return output_probs\n",
    "\n",
    "# Navigation function\n",
    "def nn_navigate_me(me, inp):\n",
    "    \"\"\"\n",
    "    Determine agent's movement based on neural network outputs.\n",
    "    \n",
    "    Args:\n",
    "        me: Agent genome (neural network weights)\n",
    "        inp: Sensory inputs\n",
    "        \n",
    "    Returns:\n",
    "        Move: 0=up, 1=down, 2=left, 3=right\n",
    "    \"\"\"\n",
    "    outputs = nn_function(inp, me)\n",
    "    \n",
    "    # Choose direction with highest output\n",
    "    return np.argmax(outputs)\n",
    "\n",
    "# Helper function to simulate a single game round\n",
    "def simulate_game_round(me, start_pos=None):\n",
    "    \"\"\"\n",
    "    Simulate a single game round for an agent.\n",
    "    \n",
    "    Args:\n",
    "        me: Agent genome\n",
    "        start_pos: Starting position (random if None)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (score, steps_taken, reached_goal)\n",
    "    \"\"\"\n",
    "    # Initialize game with random or specified starting position\n",
    "    if start_pos is None:\n",
    "        pos = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "    else:\n",
    "        pos = start_pos.copy()\n",
    "    \n",
    "    # Skip if starting at goal (retry with a different position)\n",
    "    if pos == GOAL_POS:\n",
    "        return simulate_game_round(me)\n",
    "    \n",
    "    steps = 0\n",
    "    previous_positions = []  # Track visited positions to detect loops\n",
    "    reached_goal = False\n",
    "    \n",
    "    # Simulate game round\n",
    "    while steps < MAX_STEPS and not reached_goal:\n",
    "        # Sense environment\n",
    "        inp = sense_environment(pos, GOAL_POS, OBSTACLES)\n",
    "        \n",
    "        # Decide move\n",
    "        move = nn_navigate_me(me, inp)\n",
    "        \n",
    "        # Store current position before moving\n",
    "        previous_positions.append(tuple(pos))\n",
    "        \n",
    "        # Update position based on selected move\n",
    "        new_pos = pos.copy()\n",
    "        if move == 0 and pos[1] < GRID_SIZE-1:  # up\n",
    "            new_pos[1] += 1\n",
    "        elif move == 1 and pos[1] > 0:  # down\n",
    "            new_pos[1] -= 1\n",
    "        elif move == 2 and pos[0] > 0:  # left\n",
    "            new_pos[0] -= 1\n",
    "        elif move == 3 and pos[0] < GRID_SIZE-1:  # right\n",
    "            new_pos[0] += 1\n",
    "        \n",
    "        # Check if new position is an obstacle\n",
    "        if OBSTACLES and [new_pos[0], new_pos[1]] in OBSTACLES:\n",
    "            # Stay in current position if would hit obstacle\n",
    "            pass\n",
    "        else:\n",
    "            # Move to new position\n",
    "            pos = new_pos\n",
    "        \n",
    "        # Check if goal reached\n",
    "        if pos == GOAL_POS:\n",
    "            reached_goal = True\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        # Detect loops (revisiting same position more than twice)\n",
    "        if previous_positions.count(tuple(pos)) > 2:\n",
    "            break  # Penalize looping behavior\n",
    "    \n",
    "    # Calculate score based on performance\n",
    "    if reached_goal:\n",
    "        # Reward for reaching goal (inversely proportional to steps)\n",
    "        base_score = MAX_STEPS * 2  # Base score for reaching goal\n",
    "        speed_bonus = MAX_STEPS - steps  # Bonus for reaching quickly\n",
    "        score = base_score + speed_bonus\n",
    "    else:\n",
    "        # Score based on Manhattan distance to goal if goal not reached\n",
    "        dist = abs(pos[0] - GOAL_POS[0]) + abs(pos[1] - GOAL_POS[1])\n",
    "        max_dist = GRID_SIZE * 2  # Maximum possible distance\n",
    "        score = max(0, max_dist - dist)  # Higher score for closer positions\n",
    "        \n",
    "        # Penalize for revisiting positions (loops)\n",
    "        loop_penalty = sum(previous_positions.count(p) - 1 for p in set(previous_positions))\n",
    "        score = max(1, score - loop_penalty)  # Ensure minimum score of 1\n",
    "    \n",
    "    return score, steps, reached_goal\n",
    "\n",
    "# Fitness function\n",
    "def handle_mes_fitnesses(mes):\n",
    "    \"\"\"\n",
    "    Compute fitness for a list of agents based on multiple game rounds.\n",
    "    \n",
    "    Args:\n",
    "        mes: List of agent genomes\n",
    "        \n",
    "    Returns:\n",
    "        List of fitness values as tuples\n",
    "    \"\"\"\n",
    "    fitnesses = []\n",
    "    \n",
    "    for me in mes:\n",
    "        total_score = 0\n",
    "        goals_reached = 0\n",
    "        \n",
    "        # Run multiple game rounds from different starting positions\n",
    "        for _ in range(N_GAME_ROUNDS):\n",
    "            # Generate start position away from goal\n",
    "            start_pos = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "            while start_pos == GOAL_POS:  # Ensure not starting at goal\n",
    "                start_pos = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "            \n",
    "            # Simulate game round\n",
    "            score, steps, reached_goal = simulate_game_round(me, start_pos)\n",
    "            \n",
    "            total_score += score\n",
    "            if reached_goal:\n",
    "                goals_reached += 1\n",
    "        \n",
    "        # Average score across game rounds\n",
    "        avg_score = total_score / N_GAME_ROUNDS\n",
    "        \n",
    "        # Add goal-reaching bonus to fitness\n",
    "        final_fitness = avg_score + (goals_reached * 20)\n",
    "        \n",
    "        fitnesses.append((final_fitness,))\n",
    "    \n",
    "    return fitnesses\n",
    "\n",
    "# Initialize DEAP toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "# Register functions for generating genes, individuals, and population\n",
    "toolbox.register(\"gene_maker\", random.uniform, -1, 1)  # weights in [-1, 1]\n",
    "toolbox.register(\"individual_creator\", tools.initRepeat, creator.Individual, \n",
    "                 toolbox.gene_maker, n=GENOME_LENGTH)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual_creator)\n",
    "\n",
    "# Register genetic algorithm operators\n",
    "toolbox.register(\"evaluate\", lambda ind: handle_mes_fitnesses([ind])[0])\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutGaussian, mu=0, sigma=0.3, indpb=MUTATION_INDPB)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=TOURNAMENT_SIZE)\n",
    "\n",
    "# Initialize statistics tracking\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean)\n",
    "stats.register(\"min\", np.min)\n",
    "stats.register(\"max\", np.max)\n",
    "stats.register(\"std\", np.std)\n",
    "\n",
    "def visualize_best_agent(genome, n_trials=3):\n",
    "    \"\"\"\n",
    "    Visualize the behavior of the best agent in the grid world.\n",
    "    \n",
    "    Args:\n",
    "        genome: Genome (weights) of the best agent\n",
    "        n_trials: Number of trials to visualize\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 5*n_trials))\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        # Initialize random starting position (away from goal)\n",
    "        start_pos = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "        while start_pos == GOAL_POS:\n",
    "            start_pos = [random.randint(0, GRID_SIZE-1), random.randint(0, GRID_SIZE-1)]\n",
    "        \n",
    "        pos = start_pos.copy()\n",
    "        path = [pos.copy()]\n",
    "        steps = 0\n",
    "        reached_goal = False\n",
    "        \n",
    "        # Simulate agent's path\n",
    "        while steps < MAX_STEPS and not reached_goal:\n",
    "            inp = sense_environment(pos, GOAL_POS, OBSTACLES)\n",
    "            move = nn_navigate_me(genome, inp)\n",
    "            \n",
    "            # Update position\n",
    "            new_pos = pos.copy()\n",
    "            if move == 0 and pos[1] < GRID_SIZE-1:  # up\n",
    "                new_pos[1] += 1\n",
    "            elif move == 1 and pos[1] > 0:  # down\n",
    "                new_pos[1] -= 1\n",
    "            elif move == 2 and pos[0] > 0:  # left\n",
    "                new_pos[0] -= 1\n",
    "            elif move == 3 and pos[0] < GRID_SIZE-1:  # right\n",
    "                new_pos[0] += 1\n",
    "            \n",
    "            # Check if new position is an obstacle\n",
    "            if OBSTACLES and [new_pos[0], new_pos[1]] in OBSTACLES:\n",
    "                pass  # Stay in current position\n",
    "            else:\n",
    "                pos = new_pos\n",
    "                \n",
    "            path.append(pos.copy())\n",
    "            \n",
    "            if pos == GOAL_POS:\n",
    "                reached_goal = True\n",
    "                \n",
    "            steps += 1\n",
    "        \n",
    "        # Plot the grid and path\n",
    "        plt.subplot(n_trials, 1, trial+1)\n",
    "        plt.grid(True)\n",
    "        plt.xlim(-0.5, GRID_SIZE-0.5)\n",
    "        plt.ylim(-0.5, GRID_SIZE-0.5)\n",
    "        \n",
    "        # Draw grid\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                plt.plot([i-0.5, i+0.5], [j-0.5, j-0.5], 'k-', alpha=0.2)\n",
    "                plt.plot([i-0.5, i-0.5], [j-0.5, j+0.5], 'k-', alpha=0.2)\n",
    "                plt.plot([i+0.5, i+0.5], [j-0.5, j+0.5], 'k-', alpha=0.2)\n",
    "                plt.plot([i-0.5, i+0.5], [j+0.5, j+0.5], 'k-', alpha=0.2)\n",
    "        \n",
    "        # Draw obstacles if any\n",
    "        if OBSTACLES:\n",
    "            for ox, oy in OBSTACLES:\n",
    "                plt.fill([ox-0.5, ox+0.5, ox+0.5, ox-0.5], \n",
    "                         [oy-0.5, oy-0.5, oy+0.5, oy+0.5], 'gray', alpha=0.5)\n",
    "        \n",
    "        # Draw start and goal\n",
    "        plt.plot(start_pos[0], start_pos[1], 'go', markersize=10, label='Start')\n",
    "        plt.plot(GOAL_POS[0], GOAL_POS[1], 'ro', markersize=10, label='Goal')\n",
    "        \n",
    "        # Draw path\n",
    "        path_x = [p[0] for p in path]\n",
    "        path_y = [p[1] for p in path]\n",
    "        plt.plot(path_x, path_y, 'b-', linewidth=2, alpha=0.6)\n",
    "        plt.plot(path_x, path_y, 'b.', markersize=5)\n",
    "        \n",
    "        # Add labels\n",
    "        plt.title(f'Trial {trial+1}: {\"Goal Reached\" if reached_goal else \"Failed\"} in {len(path)-1} steps')\n",
    "        plt.xlabel('X')\n",
    "        plt.ylabel('Y')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create initial population\n",
    "    population = toolbox.population(n=POPULATION_SIZE)\n",
    "    \n",
    "    # Evaluate fitness for all individuals\n",
    "    fitnesses = handle_mes_fitnesses(population)\n",
    "    for ind, fit in zip(population, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "    \n",
    "    # Create hall of fame to store top individuals\n",
    "    hof = tools.HallOfFame(ELITISM_SIZE)\n",
    "    hof.update(population)\n",
    "    \n",
    "    # Track fitness history for plotting\n",
    "    fitness_history = []\n",
    "    gen_history = []\n",
    "    \n",
    "    # Main evolutionary loop\n",
    "    for gen in range(MAX_GENERATIONS):\n",
    "        print(f\"-- Generation {gen} --\")\n",
    "        \n",
    "        # Select the next generation individuals\n",
    "        offspring = toolbox.select(population, len(population) - ELITISM_SIZE)\n",
    "        \n",
    "        # Clone the selected individuals\n",
    "        offspring = list(map(toolbox.clone, offspring))\n",
    "        \n",
    "        # Apply crossover and mutation\n",
    "        for i in range(0, len(offspring), 2):\n",
    "            if i+1 < len(offspring) and random.random() < CROSSOVER_RATE:\n",
    "                offspring[i], offspring[i+1] = toolbox.mate(offspring[i], offspring[i+1])\n",
    "                del offspring[i].fitness.values\n",
    "                del offspring[i+1].fitness.values\n",
    "        \n",
    "        for i in range(len(offspring)):\n",
    "            if random.random() < MUTATION_RATE:\n",
    "                offspring[i], = toolbox.mutate(offspring[i])\n",
    "                del offspring[i].fitness.values\n",
    "        \n",
    "        # Add elites from previous generation\n",
    "        elites = toolbox.clone(tools.selBest(population, ELITISM_SIZE))\n",
    "        \n",
    "        # Evaluate the individuals with an invalid fitness\n",
    "        invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "        fitnesses = handle_mes_fitnesses(invalid_ind)\n",
    "        for ind, fit in zip(invalid_ind, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        # Replace population with offspring + elites\n",
    "        population[:] = offspring + elites\n",
    "        \n",
    "        # Update hall of fame\n",
    "        hof.update(population)\n",
    "        \n",
    "        # Gather statistics\n",
    "        record = stats.compile(population)\n",
    "        print(f\"  Min: {record['min']:.2f}, Max: {record['max']:.2f}, Avg: {record['avg']:.2f}, Std: {record['std']:.2f}\")\n",
    "        \n",
    "        # Record max fitness for this generation\n",
    "        gen_history.append(gen)\n",
    "        fitness_history.append(record['max'])\n",
    "    \n",
    "    # Extract best individual\n",
    "    best_individual = tools.selBest(population, 1)[0]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n=== Best Individual ===\")\n",
    "    print(f\"Fitness: {best_individual.fitness.values[0]:.2f}\")\n",
    "    \n",
    "    # Plot fitness evolution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gen_history, fitness_history, linewidth=2)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Max Fitness (Score)')\n",
    "    plt.title('Evolution of Neural Network Fitness')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize best agent's behavior\n",
    "    print(\"\\nVisualizing best agent's behavior...\")\n",
    "    visualize_best_agent(best_individual)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
